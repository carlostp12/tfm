\phantomsection
\pagestyle{fancy}

\chapter{State of the art}\label{sec:State_of_art}
\onehalfspacing
%\addcontentsline{toc}{chapter}{State_of_art}
%\section{Introduction}

This chapter establishes the academic context for the research area addressed in this work. It begins by defining the primary subjects of study—galaxy clusters and groups —followed by a comprehensive overview of the redshift surveys that provide the necessary data. Furthermore, it details the redshift-distance relation as a mechanism for converting redshifts into physical distances using modern cosmological models. The chapter also introduces the two-point correlation function, which provides a statistical framework for analyzing matter density. Finally, it concludes with a concise review of the unsupervised Machine Learning techniques utilized throughout this study.

\section{Galaxy groups and clusters: Primary targets for structural identification}

Galaxy groups and clusters \ref{fig:cluster} are the largest gravitationally structures in the Universe and are crucial probes of the underlying cosmic dark matter density field. They both consist of dark matter halos containing multiple galaxies, their distinction is typically based on mass and membership:

\begin{enumerate}

	\item \textit{Galaxy Groups}: These are the most common and lowest-mass virialized systems, typically containing 3 to 50 member galaxies \cite{Hai-Xia-Ma:2025} and spanning a total mass range of $\sim 10^{13} - 10^{14} M_{\odot}$. Our own Local Group is a well-known example.

	\item \textit{Galaxy Clusters}: These represent the high-mass tail of the halo distribution, typically containing hundreds to thousands of galaxies, with total masses ranging from $\sim 10^{14} - 10^{15} M_{\odot}$. They often host a dominant, massive Brightest Cluster Galaxy (BCG) at their center and are strong emitters of X-rays and which properties dictate cluster formation and evolution \cite{Hai-Xia-Ma:2025}. A paradigmatic example is the Virgo cluster with an estimate mass of $\sim 1.2×1015 M_{\odot}$ and having M87 as most massive and dynamically dominant central galaxy.
	
\end{enumerate}

%\begin{figure}
%\centering
%\begin{minipage}{.5\textwidth}
%  \centering
%  \includegraphics[width=.5\linewidth]{./figs/M33_2.jpg}
%  \caption{M33 the $3^{th}$ galaxy in mass in our local group. Source \cite{eso:2025}}
%  \label{fig:group}
%\end{minipage}%
%\begin{minipage}{.5\textwidth}
%  \centering
%  \includegraphics[width=.5\linewidth]{./figs/virgo_cluster.jpg}
%  \caption{Portion of Virgo cluster. Source \cite{eso:2025}}
%  \label{fig:cluster}
%\end{minipage}
%\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=.75\linewidth]{./figs/virgo_cluster.jpg}
  \caption{Portion of Virgo cluster}. Source \cite{eso:2025}
  \label{fig:cluster}
\end{figure}

Because both groups and clusters exhibit diverse morphologies and spatial shapes, the subsequent analysis —detailed in Section \ref{unsupervised_methods}— employs unsupervised density-based algorithms. These methods are the most suitable approach for the clustering analysis due to their capacity to effectively fit structures with arbitrary geometries.

\section{Spectroscopic  Surveys}

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{./figs/survey_2dmap_small-old2.jpg}
	\caption{2dfGRS sky coverage}. Source \cite{Colless:2001}.
	\label{fig:2dfGRS}
	\end{figure}
	
Spectroscopic surveys are fundamental projects in astronomy and cosmology that collect and analyze the spectrum of light from a large number of celestial objects over a wide area of the sky. By splitting the light into its constituent wavelengths, these surveys acquire a vast amount of detailed physical information for each object.

One main purpose of this kind of studies is to obtain a highly precise redshift ($z$) of each object in order to estimate distances. Therefore it is possible to construct a three-dimensional map of the Universe.

The spectrum serves as well as a unique fingerprint of the source, allowing for the determination of its physical properties. Spectral analysis provides detailed information on the object's chemical composition, temperature, density, and internal motion.

We are fortunate that, nowadays, we have access to data from several major astronomical surveys, including, but not limited to, the following:

\subsection{2dF Galaxy Redshift Survey (2dfGRS)}

2dfGRS is a Survey leveraged the unique capabilities of the 2dF (2-degree Field) facility built by the Anglo-Australian Observatory in the southern hemisphere (see in figure \ref{fig:aao}). A view of 2dfGRS coverage is shown in figure \ref{fig:2dfGRS}. Data was collected between 1997 and 2004.



	\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{./figs/SSO_2.jpg}
	\caption{Australian Astronomical Observatory (AAO)}. Source \cite{aao:2000}.
	\label{fig:aao}
	\end{figure}
	
The data set employed in this analysis originates from the 2003 final data release, which encompasses a total of 245,591 objects. After quality cuts, 221,414 objects were determined to be spectroscopically reliable galaxy data, thus forming the foundation for the subsequent investigation.
	
\subsection{Sloan Digital Sky Survey (SDSS)} \label{data:sdss}

The Sloan Digital Sky Survey (SDSS) \cite{sdss:2000} began collecting data in 2000 and is one of the largest and most influential astronomical surveys ever conducted. The primary goal is to comprehensively map the Universe to expand our understanding of its large-scale structure, the formation of stars and galaxies, and the history of the Milky Way. It uses a dedicated wide-angle optical telescope (the Sloan Foundation 2.5-m Telescope) at Apache Point Observatory in New Mexico (see figure \ref{fig:sdss}), and in later phases, also observations in the Southern Hemisphere.

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figs/apo_wide.jpg}
	\caption{Apache Point Observatory in New Mexico.} Source \cite{aao:2000}.
	\label{fig:sdss}
	\end{figure}
	
The SDSS has progressed through several phases (SDSS-I, II, III, IV, and the current SDSS-V), with each phase introducing new scientific goals and technological advancements. For this work the modelC petrosian magnitude data realease 7 (DR7) will be used, which contains 639359 galaxy entries. This release offers coverage for approximatyely one quarter of the sky sphere, predominantely in the nothern galactic cap as illustrated in figure \ref{fig:dr7}. Groups constructed are drawn up from \cite{Yang:2007}.

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{./figs/SDSSDR7-coverage2.jpg}
	\caption{SDSS Data release 7 sky coverage.} Source \cite{Yang:2007}.
	\label{fig:dr7}
	\end{figure}


\subsection{Inherent challenges associated with survey-collected data} \label{challenges}

Beyond the substantial logistical and resource constraints associated with constructing and operating large-scale astronomical facilities, all galaxy redshift surveys are subject to a distinct set of fundamental systematic and technical challenges. They can be broadly categorized into those arising from instrumental and observational limitations, and those derived from subsequent collecting data and analysis:

\begin{enumerate}
	\item Observational and measurement errors: produced by instrumental limitations due long time exposures, observing faint objects in high redshifts where long integration times are required to achieve an adequate signal-to-noise ratio (SNR). Additional noise is introduced by environmental factors, such as atmospheric distortion (seeing) or the inherent difficulty of performing accurate sky subtraction during data processing.
	
	\item Systematic errors and biases, for example the already mentioned distorsion on redshifts caused by peculiar velocities of galaxies. This effect leads to the apparent elongation of clusters along the line of sight, a well-known phenomenon often referred to as the "Fingers of God" effect \cite{Longair:1996}. There is also a bias caused by the galaxy type, luminosity or epoch of the Universe can also lead to innacurate outcomes in the survey.
	
	\item Theoretical and modeling uncertainties represent another category of challenges, primarily stemming from the interpretation of observational data. These uncertainties arise both from limitations in the underlying theoretical models and from poorly constrained baryonic effects that impact dark matter simulations.
\end{enumerate}

\subsection{Galaxy cluster/group catalog}

In order to validate and assess the performance of a constructed galaxy clustering model, it is necessary to compare the results from the target galaxy catalog (the data being clustered) against a well-established, pre-existing group/cluster catalog (the ground truth). For this work was employed the catalog result obtained by the halo-based group finder developed by in \cite{Yang:2007}. This method is specifically optimized for grouping galaxies residing in the same dark matter halo and and utilizes an iterative approach based on an adaptive filter modeled after the general properties of dark matter halos (see \cite{Yang:2007}) for full details).

The halo-based group finder was successfully applied to the galaxy catalogs from both the 2dFGRS and the SDSS (Data Release 7). This valuable group catalog is publicly accessible and can be downloaded from \cite{groups:2000}.

\section{The redshift–distance relation}

It is well-established that the Universe is undergoing cosmic expansion. The Hubble–Lemaître Law quantifies this expansion, stipulating that the recessional velocity of a galaxy is directly proportional to its proper distance from the observer. This relationship is described by the equation:

\begin{equation} \label{eq:1}
	V=H_{0}D
\end{equation}	
Where $V, H_{0}, D$ are respectively, the velocity, Hubble constant and distance.

 The equation \ref{eq:1} is strictly valid on small redshifts $ z \ll 1 $ \cite{Cepa:2023} (which means nearby objects). In higher redshift it is necessary to use a full cosmological model to address the redshift-distance relation. According to the most recent theories \cite{Cepa:2023}:

\begin{equation} \label{eq:2}
	D_p =  \frac{1}{H_0} \int_0^z \frac{dz}{ \sqrt{\sum_i \Omega_{r0} (1+z)^{4} + \Omega_{m0} (1+z)^{3} + \Omega_{\Lambda 0}  } }
\end{equation}.

Where $\Omega_{r0}, \_, \Omega_{m0}, \_ \Omega_{\Lambda 0}$ represents the density parameters for radiation, mass and dark energy (respectively) in the present epoch $z=0$.

The specific form of equation \ref{eq:2} may vary according to the chosen cosmological model. In this work it is assumed the values of the $\Lambda\text{CDM}$, according to \cite{Cepa:2023}:

\begin{equation} \label{eq:3}
 \Omega_{r0} = 0.0001, \,\, \Omega_{m0} = 0.3 , \,\, \Omega_{\Lambda 0} = 0.7.
\end{equation}.

\subsection{Real space galaxy catalog}\label{section:real}

Redshift surveys suffer distortions that are primarily categorized into two distinct sets:

\begin{enumerate}
	\item FOG (Finger Of God): reduction in the correlation produced on small scales by the virialized motion of galaxies within the dark matter halos.
	\item Kaiser effect: boost of the correlation in large scales induced by the infall motion of galaxies towards overdensity regions.
\end{enumerate}

Both categories of distortions manifest within the correlation function of the galaxy distribution and the global matter distribution. 

Several accurate methodologies are available to mitigate the effects of both issues, at least partially. This discussion will focus on the approach described by Shi et al. in \cite{Shi:2016}, where the authors propose a method to correct these distortions as follows:

The observed redshift ($z_{\text{obs}}$) of a galaxy is a combination of the cosmological redshift ($z_{\text{cos}}$) due to the Hubble flow and the Doppler shift caused by the galaxy's peculiar velocity ($v_{\text{pec}}$).4 The total relationship is expressed as:

\begin{equation} \label{eq:4}
 z_{obs} = z_{cos} + z_{pec} = z_{cos} + \frac{v_{pec}}{c}(1+z_{cos}).
\end{equation}.

And then for $v_{\text{pec}}$:

\begin{equation} \label{eq:5}
 v_{pec} = v_{cen} + v_{\sigma}.
\end{equation}.

Where $v_{\text{cen}}$ is the line-of-sight component of the coherent velocity of the halo center, representing the infall motion that causes the large-scale Kaiser effect. $v_{\sigma}$ is the line-of-sight component of the internal velocity dispersion of the galaxy with respect to its halo center, representing the random motion that causes the small-scale Fingers-of-God (FoG) effect.

The resulting effective redshifts corresponding to these two distinct effects are formally defined as follows:

\begin{equation} \label{eq:7}
 z_{FOG} = z_{cos} + \frac{v_{\sigma}}{cen}(1+z_{cos}).
\end{equation}.

Roughly speaking $v_{cen}$ is the manifestation of  $z_{Kaiser}$, whereas FOG effect is depicted by $v_{\sigma}$. 

Following the methodology proposed in \cite{Shi:2016}, the authors define several coordinate spaces by utilizing Equations \ref{eq:4}, \ref{eq:5}, and \ref{eq:7} to recalculate the positions (Equation \ref{eq:2}) in each space.

Based on these coordinate frameworks, a transformation is applied to the original redshift-space data to obtain a new system known as \textbf{Re-Real space} \cite{Shi:2016}. This space is particularly relevant as it mitigates the impact of both Kaiser and Finger-of-God (FoG) distortions. In this study, the adoption of Re-Real space is fundamental to improving the performance of density-based algorithms, as it restores the physical isotropy of galaxy groups.

Employing this approach yields a modified catalog where the new redshift values effectively transform the coordinate space, as illustrated in Figure \ref{fig:real}. By using this Re-Real space dataset will help to thoroughly investigate how this transformation influences the resulting clustering of galaxies.

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{./figs/real_space.jpg}
	\caption{The Re-Real space.} In the left-hand side a sample of SDSS in the redshift space. The right-hand side shows SDSS in Re-Real space. Source \cite{Shi:2016}.
	\label{fig:real}
	\end{figure}
	
\section{The two-point correlation function: a statistical point of view in density analysis}\label{2pcf1}

An alternative approach to investigating the cosmic galaxy density field is through the two-point correlation function (2PCF). This statistic provides a robust measure for quantifying the spatial clustering of a galaxy distribution by determining the excess probability of finding a pair of objects at a given separation scale, $r$, compared to a random (Poisson) distribution.

Unlike clustering algorithms which performs a discrete classification (deciding which galaxy belongs to which group), 2PCF offers a global statistical analysis of the distribution (measuring how galaxies "crowd" together across the entire manifold). This is done by quantify the probability of finding galaxy pairs at specific separation scales across the cosmic manifold.

The two-point correlation function $\xi(r)$ and the power spectrum, $P(k)$, form a Fourier transform pair, representing the clustering signal in real and spectral space, respectively. While $\xi(r)$ provides an intuitive measure of spatial separations, the power spectrum is the standard framework for characterizing the primordial density fluctuations observed in the Cosmic Microwave Background (CMB).x

According to \cite{Peebels:1980}, if one pick a galaxy at random, the probability ($dP$) of finding another galaxy at a distance $r$ within a small volume $dV$ is given by:

$$dP = \bar{n}^2 [1 + \xi(r)] dV_1 dV_2$$

Where $\bar{n}$ is the mean number density of galaxies in the survey and $\xi(r)$ is the correlation function.

The value of $\xi(r)$ tells us how the galaxies "feel" each other at different scales \cite{Peebels:1980}: 
\begin{enumerate}
	\item $\xi(r) > 0$ (Clustering): You are more likely to find a pair of galaxies at that distance than you would by pure chance. On small scales, gravity pulls galaxies together, making this value high.
	
	\item$\xi(r) = 0$ (Randomness): The distribution is perfectly random (like static on a TV).
	
	\item $\xi(r) < 0$ (Anti-clustering): Galaxies are "repelling" each other or are more spread out than a random distribution.
\end{enumerate}	

Drawing on the comparative analysis provided by \cite{Kerscher:2000}, four distinct estimators to quantify the clustering signal was implemented in this work. They are:

Natural:
\begin{equation} \label{eq:43}
	\widehat{\xi} _{N}= \frac{DD}{RR} - 1
\end{equation}.

Davids and Peebels:
\begin{equation} \label{eq:41}
	\widehat{\xi} _{DP}= \frac{DD}{DR} - 1
\end{equation}.

Hamilton:
\begin{equation} \label{eq:42}
	\widehat{\xi} _{Ha}= \frac{DD \, RR}{DR^2}
\end{equation}.

Landy and Szalay:
\begin{equation} \label{eq:44}
	\widehat{\xi} _{LS}= \frac{DD -2DR + RR}{RR}
\end{equation}.
	
\section{Machine Learning applied to cosmology}

A brief description of Machine Learning algorithms will be described, emphasizing those used in this work.

\subsection{Supervised methods}

Supervised learning focuses on identifying patterns and relationships within labeled datasets. The primary objective of supervised methods is to extract knowledge from the given training data to enable accurate class predictions for new, unseen data. Formally, given a labeled dataset $Z = (X, Y)$, where $X = (X_1, \dots, X_n)$ are the input features and $Y = (Y_1, \dots, Y_m)$ are the corresponding labels, the goal is to find a function $F$ such that the relationship $Y = F(X)$ is approximated.

A subset is taken from the original dataset, the so called training data \( Z_i = (X_i, Y_i) \). And then the problem is reduced to find the minumum of a loss function, which measures the difference between \( Y_i \) and \( F(X_i) \).

The input to any supervised algorithm consists of independent variables (or features), and the output comprises the dependent variables (or target variables). Supervised algorithms leverage the information within the training data to learn the intricate relationships between these input and target variables.

However, a detailed discussion of supervised methods is not the scope of this work. Tnstead, The objective is to identify patterns and structure within the data distribution, which will subsequently inform the spatial distribution of matter within a dimensional space.

\subsection{Unsupervised methods}\label{unsupervised_methods}

Unsupervised learning focuses on analysis and modeling of data that lack output classes or pre-existing labels. This methodology aims to discover intrinsic structure, patterns, and relevant features within the data itself.

Formally, the input consists of a set of observations (or data points) where the feature matrix $X$ is given by $X = (X_1, \dots, X_n)^T$. The primary objective is to learn the underlying distribution or to find meaningful representations from these input variables without any prior guidance.

Clustering and segmentation belong to Unsupervised methods, that work based on distance and similarity patterns can be divided as follows:

\begin{itemize}
	\item \textit{Hierarquical}: this method creates successive partitions of data and a hierarchical tree, called a dendrogram. Examples include agglomerative clustering. 
	\item \textit{Partitional}: an initial set of clusters must be set in advance, the set is improved on an iterative proccess. Example $k$-means.
	\item \textit{Model-Based}: these algorithms assume that the data is generated by a mixture of underlying probability distributions (e.g., Gaussian Mixture Models, GMM).
	\item \textit{Density-based}: this method defines clusters as contiguous regions of high density separated by regions of low density (e.g., DBSCAN).
	
\end{itemize}

	The key advantage of density-based methods is the fundamental lack of a priori assumptions regarding the underlying data distribution. These algorithms operate by defining clusters as contiguous, dense regions of data points that are separated by sparser areas. This characteristic makes them highly suitable for exploratory data analysis, as they impose no constraints on the shape of the resultant clusters.
	
	A further feature of density-based methods is their intrinsic ability to detect outliers or noise points. These points typically reside in the low-density regions that naturally separate the dense clusters, allowing for robust identification of anomalous observations without a dedicated process.
	
	Conversely, many hierarchical and partitional algorithms rely on strong assumptions about the data's structure. For instance, the $k$-means algorithm requires the number of clusters ($k$) to be predefined and implicitly assumes that the clusters follow a globular or spherical shape (often analogous to a Gaussian probability distribution). This inherent bias makes them unsuitable for astronomical data, where the spatial distribution of matter is expected to exhibit arbitrary, non-spherical geometries—such as linear filaments, stellar-like distributions, or complex polygonal structures. Thus, these restrictive methods are not appropriate for our analysis.
	
	For this study, a set of representative density-based algorithms: OPTICS, DBSCAN, HDBSCAN and DPC. The following section will provide a detailed overview of the mathematical foundations and operational characteristics of each.
	
\subsection{OPTICS}\label{optics}

	Namely Ordering Points to Identify Cluster Structure: is a density-based, unsupervised algorithm. Its primary mechanism involves ordering the data points based on their reachability distance relative to a specified density threshold.
	
	The output of OPTICS is not a finalized set of clusters but rather a visual tool called the reachability plot (or reachability-distance graph). This plot encodes the density structure of the dataset, from which clusters of varying density and hierarchy can be later extracted.

	Let us define the foundational geometric concepts required to understand the OPTICS algorithm.
	
	\begin{itemize}
		\item \textit{eps-neighborhood} of a point $p$ in S is \(NE_{\epsilon}(p) =\{q \in S : dist(p, q) \leq eps \} \). Then any eps-neighborhood of $p$ is said to be dense if 
		\( |NE_{\epsilon}(p)| \geq minPts \).
	
		\item The \textit{core\_distance} of a given point $p$ is the minimum \(\epsilon\) such us \(NE_{\epsilon}(p)\) is dense, in other words:
		
		$$ core-distance(p) = min \{ \epsilon : |NE_{\epsilon}(p)| \geq minPts \} $$
	
		\item A point is said to be a \textit{core-point} when \(NE_{\epsilon}'(p) \) is dense and  \( \epsilon' \leq \epsilon \), finally,
	
	
		\item The \textit{reachability-distance} from q regarding a core-point g is the maximum of the two: core-distance and Euclidean  distance, in other words:

		$$ reachability-distance(p, q) = max \{ core-distance(p), dist(p, q) \} $$
		
	\end{itemize}
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{./figs/core-distance.jpg}
	\caption{Core and reachability distances}. Source \cite{Rhys:2020}.
	\label{fig:optics_figure}
	\end{figure}
	
	Note that reachability-distance is only defined with respect to a core-point. See the \ref{fig:optics_figure} for an illustrative example of both core-distance and reachability-distance in the figure .

	OPTICS work by setting up two mandatory parameters: 
	
	\begin{enumerate}
		\item Eps ($ \epsilon $): The maximum radius to search for neighbors.
		\item  minPts: The minimum number of neighbors a point needs to have to be considered a core-point.
	\end{enumerate}
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figs/reach3.jpg}
	\caption{An example of data set in \( \mathbb{R}^2\).}
	\label{fig:reach1}
	\end{figure}
	
	For example, figure \ref{fig:reach1} shows a random-generated set of points around four fixed points within [0,1]x[0,1] square. OPTICS is then applied to this, and the resulting reachability plot is shown in figure \ref{fig:reach2}.
	
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figs/reach2.png}
	\caption{Example of OPTICS reachability plot}
	\label{fig:reach2}
	\end{figure}

\subsection{DBSCAN}

DBSCAN (Density-based Spatial Clustering of Applications with Noise) is another density-based clustering algorithm that leverages several concepts from OPTICS to efficiently extract clusters. However, DBSCAN introduces specific, additional definitions for identifying points and cluster boundaries, which are summarized below.

Given a dataset $S$, a minimum number of points $\text{MinPts}$, and a neighborhood radius $\text{Eps}$, let $p$ be a core-point of $S$. Then:

\begin{itemize}
		\item A point $q$ is defined as \textit{directly density-reachable} from a core-point $p$ if $q$ is within the $\epsilon$-neighborhood of $p$ (i.e., $q \in NE_{\epsilon}(p)$). This definition is valid only when $p$ satisfies the core-point condition: $|NE_{\epsilon}(p)| \geq \text{MinPts}$
		
		\item A point q is said to be \textit{density-reachable} with respect to Eps and MinPts if there exists an ordered sequence of points $p_1, ..., p_n$ such that:
		
		\begin{enumerate}
			\item $p_1 = p$ and $p_n = q$.
			\item $p_{i+1}$ is directly density-reachable from $p_i$ for all $1 \leq i \leq n$.
		\end{enumerate}
		
		\item The point $p$ is \textit{density-connected} to a point $q$ with respect to Eps and MinPts if there exists third point o such that both $p$ and $q$ are density-reachable from o.
\end{itemize}

The figure \ref{fig:density} illustrates both concepts: density-connectivity and density-reachability.

Then a cluster $C$ is a subset of S satisfying:

	\begin{itemize}
		\item \(\forall p, q, \in C\)  $p$ is density-connected from $q$ with respect to Eps and MinPts.
		\item \(\forall p, q, \in C\) if $q$ is density reachable from $p$ with respect to Eps and MinPts then \(q \in C \). This property is called sometimes as \textit{Maximallity}.
	\end{itemize}

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{./figs/density.jpg}
	\caption{Density Reachability and Density Ronnectivity}: Left: density reachability. Right: density connectivity. Source \cite{Rhys:2020}.
	\label{fig:density}
	\end{figure}
	
DBSCAN creates then a set of clusters $C_1, ..., C_k$ and all points in $S$ are classified as:

\begin{enumerate}
		\item \textit{Core-point}: points with a dense neighborhood.
		\item \textit{Border-point}: points belonging to a cluster but without a dense neighborhood.
		\item \textit{Noise-point}: points do not belonging to any cluster.
\end{enumerate}

The DBSCAN algorithm initiates cluster discovery by selecting an arbitrary, unvisited database point $p$ and retrieving its density-reachable neighborhood (relative to $\epsilon$ and $\text{MinPts}$). The subsequent action depends on the nature of $p$:

\begin{enumerate}
	\item If $p$ is a core-point: A new cluster is formed containing $p$ and all points density-reachable from $p$. This process is then iteratively expanded.
		
	\item If $p$ is not a core point: No points are density-reachable from $p$. DBSCAN assigns $p$ to the noise-point category and proceeds to the next unvisited point.
\end{enumerate}
		
It is important to note that if $p$ is a border-point of a cluster $C$, it will eventually be reached during the expansion phase from a core point of $C$ and correctly assigned to the cluster. The algorithm concludes once every point has been processed and assigned either to a cluster or to the noise-point set.

A DBSCAN implementation in following pseudo-code is present at next:

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwFunction{RegionQuery}{RegionQuery}
\SetKwFunction{ExpandCluster}{ExpandCluster}

\Input{Dataset $D$, $\epsilon$ (epsilon), $\text{MinPts}$ (minimum points)}
\Output{Set of clusters $C$, with noise points unassigned}

\BlankLine

$C \leftarrow 0$ \tcp{Cluster counter}
\For{each point $P$ in $D$}{
    \If{$P$ is unvisited}{
        mark $P$ as visited\;
        $NE \leftarrow \RegionQuery(D, P, \epsilon)$\;
        \If{$|NE| < \text{MinPts}$}{
            mark $P$ as \textbf{Noise}\;
        }
        \Else{
            $C \leftarrow C + 1$\;
            $\ExpandCluster(D, P, NE, C, \epsilon, \text{MinPts})$\;
        }
    }
}
\caption{The DBSCAN Algorithm}
\end{algorithm}
% \BlankLine
\begin{algorithm}[H]
\SetKwProg{Fn}{Function}{}{end}
\Fn{\ExpandCluster{$D, P, NE, C, \epsilon, \text{MinPts}$}}{
    assign $P$ to cluster $C$\;
    \For{each point $P'$ in $NE$}{
        \If{$P'$ is unvisited}{
            mark $P'$ as visited\;
            $NE' \leftarrow \RegionQuery(D, P', \epsilon)$\;
            \If{$|NE'| \geq \text{MinPts}$}{
                $NE \leftarrow NE \cup NE'$\;
            }
        }
        \If{$P'$ is not yet assigned to a cluster}{
            assign $P'$ to cluster $C$\;
        }
    }
}

\BlankLine

\Fn{\RegionQuery{$D, P, \epsilon$}}{
    \KwRet{all points $P' \in D$ such that $\text{distance}(P, P') \leq \epsilon$}
}

\caption{ExpandCluster and RegionQuery functions from DBSCAN Algorithm}
\label{alg:dbscan}
\end{algorithm}

As mentioned, the algorithm takes an unvisited point $p$ and evaluates its eps-neighborhood througth the function \textit{RegionQuery}, if it contains fewer than MinPts points $p$ is labeled as noise. Otherwise $p$ is labeled as core point algorithm expand the cluster througth the \textit{ExpandCluster} function.

\subsection{sOPTICS: Modifying the distance}\label{section:slos}

Following the application proposed by \cite{Hai-Xia-Ma:2025} a modification to the distance along de line of sight as illustrated in figure \ref{fig:slos}. This adjustment is specifically designed to mitigate the distorsion referred as \textit{Finger-of-God} $FOG$ in the section \ref{section:real}. The algorithm remains as a standard OPTICS but the distance has been modified by a new one wich work as follows, given two point $u$ and $v$ the usual Euclidean distance is calculated as:

\begin{equation} \label{eq:10}
 D^2 (u,v)=\sum_{i=1}^{3} (u_i-v_i)^2.
\end{equation}.

Instead, a new version of distance concept is created by calculate the so called Ellongated Euclidean Distance as:

\begin{equation} \label{eq:11}
 D_{Elongated}^2 (u,v, sLos)= d_{traverse}^2 (u,v) + d_{sLOS}^2 (u,v, sLos) 
\end{equation}.

Where 

\begin{equation} \label{eq:12}
 d_{sLOS} (u,v, sLos) = sLOS*\frac{\sum_{i=1}^{3} (u_i-v_i)u_i}{\sqrt{\sum_{i=1}^{3}u_i^2}}
\end{equation}.

Since the sLOS factor must be calculated, the final consideration addresses the distance metric. To ensure a symmetric distance concept—and thereby guarantee the stability of the core-point definitions—the distance chosen is defined as:

 \begin{equation} \label{eq:12}
 d_{sLOS}^{sym} (u,v, sLos) = sLOS*\frac{d_{sLOS} (u,v, sLos) + d_{sLOS} (v,u, sLos)}{2}
\end{equation}.

Figure \ref{fig:slos} illustrates the elongated Euclidean distance’s effect on the OPTICS clustering results. Clusters are computed in an elongated way along the line of sight in order to cure the redshift space distortion.

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./figs/slos2.jpg}
	\caption{Deformation along the line-of-sight}: Illustration of how sLos algorithm works elongating the line of sigth. Source \cite{Hai-Xia-Ma:2025}.
	\label{fig:slos}
	\end{figure}
\subsection{HDBSCAN}

This is another option to perform unsupervised density-based clustering.

HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) is an extension of DBSCAN that transforms the density-based approach into a hierarchical clustering algorithm. It requires only one mandatory parameter: $min\_cluster\_size$ (which is equivalent to $minPts$ or the minimum size of a dense region).

HDBSCAN introduces a concept of hierarchy of clusters, first it works by estimate the new concept of \textit{mutual reachability distance} between two gien points, $p$ and $q$:

$$ mreach(p,\,q) = max{core-dist(p), \, core-dist(q), \, dist(p, q)}$$


	\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{./figs/MST.jpg}
	\caption{Minimium Spanning Tree ($MSP$)}. Source \cite{hdbscan:2000}.
	\label{fig:mst}
	\end{figure}
	
Remember from the \ref{optics} that within $core-dist$ concept depends directly on the minPts parameter. HDBSCAN uses this new concept of distance to guess dense areas in order to find clusters. First HDBSCAN calculates all mutual reachability distances, then points are placed as nodes in a graph called \textit{Minimum Spanning Tree}, $MST$ \cite{Campello:2013}, and joined by edges representing weights of their mutual reachability distances. This $MST$ is the minimum set of edges that connect points and minimizes the sum of edge weights (in fact the reachability distances). A $MST$ is shown at \ref{fig:mst}.


	
The Minimum Spanning Tree (MST), constructed using the mutual reachability distance, forms the basis for the hierarchical cluster tree (dendrogram). This hierarchy is generated by iteratively grouping points based on increasing edge weights (mutual reachability distances),
where each edge weight represents the density level at which two components become connected. The merged sets at each step constitute the cluster structure across all possible density thresholds ($\epsilon$). The final hierarchy is then simplified through a condensation process based on the user-defined parameter, $minPts$ (or $min\_cluster\_size$). The algorithm traverses  then the complete hierarchy. If a cluster splits into two new clusters, and one of the resulting clusters contains fewer than $minPts$ data points, that split is deemed insignificant. 

Thus clusters with less than minPts are treated as single clusters, the process is one of re-labeling and pruning to simplify the tree based on persistence:, turning the hierarchy less complex and more interpretable.

The final clusters are extracted from this condensed dendrogram \ref{fig:tree} by identifying the more stable clusters (most persistent) across varying density thresholds, the clusters are selected by longest lifetime $\lambda$, is the inverse of the distance (or density) at which a cluster merges or splits.

	\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\textwidth]{./figs/hbclusters.jpg}
	\caption{Condensed tree from HDBSCAN}. Source \cite{hdbscan:2000}.
	\label{fig:tree}
	\end{figure}
	
Employs Stability for Final Selection: Instead of relying on a cutoff distance, HDBSCAN calculates the cluster persistence (often called "Excess of Mass")\cite{Campello:2013} for every potential cluster in the hierarchy. It then extracts the clusters that are the most stable (exist over the largest range of density thresholds), which allows it to naturally identify and separate clusters of different local densities."

The strength of HDBSCAN is its adaptability, this can result in a defect because the ability to identify sparse areas as distinct clusters might lead to the spurious detection of minor over-densities that might be categorized as noise in our galaxy catalogs. Despite this potential ambiguity, HDBSCAN is employed in this study because its fundamental mechanism is precisely aligned with the requirements of an unsupervised density-based approach.

\subsection{Density Peaks Clustering (DPC)}

Density Peaks Clustering (DPC) is a newer (proposed in 2014) density-based algorithm, By analyzing the Decision Diagram ($\rho$ vs $\delta$), one expect to be able to isolate cluster cores from the background field population.

The Density Peak Clustering (DPC) algorithm operates under the fundamental assumption that cluster centers correspond to points where the local density attains its maximum. A critical distinguishing characteristic is that each center is separated by a considerable distance from all other centers, while being immediately surrounded by points of comparatively lower local density. Therefore, two quantities are calculated for each point, $p_i$, in the dataset:

\begin{enumerate}
	\item Local density ($\rho_{i} $). Which is calculated as $\rho_{i}=\sum_{j}\chi(d_{ij}-d_c) $, where $d_c$ is a cutoff distances and $\chi(x)=1 $ if $x<0$ and  $\chi(x)=0 $ if $x \geq 0$. In other words, $\rho_{i}$ is the number of points that are closer than $d_{c}$ to the point $i$.
	\item Distance from  next point $\delta_i$ with higher density. This parameter is measured by computing:
	$\delta_i = min_{j:\rho_j>\rho_i}(d_{ij})$. and for the point with highest density $\delta_i = max_{j}(d_{ij})$. Therefore $\delta_i$ is much larger than other typical values for those points where density reachs its local or global maximum.
\end{enumerate}
	
Following the computation of both parameters, a diagram as presented in figure \ref{fig:mst} can be utilized to establish appropriate thresholds for $\rho$ and $\delta$ in order to select the points which represent centers for each clusters$\delta$. This process allows for the identification of points representing the center of each cluster. DPC excels at cluster center detection because these selected points inherently correspond to locations where the local density reaches its maximum.

%\begin{figure}
%\centering
%\begin{minipage}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{./figs/dpc_11.jpg}
%  \caption{Different colors correspond to different clusters.}
%  \label{fig:group}
%\end{minipage}%
%\begin{minipage}{.5\textwidth}
%  \centering
%  \includegraphics[width=.8\linewidth]{./figs/dpc_12.jpg}
%  \caption{(B) Decision graph for data. Source \cite{DPC:2014}.}
%  \label{fig:cluster}
%\end{minipage}
%\end{figure}

Then it is possible to select $\rho = \rho_{0}$ and $\delta = _{0}$ to obtains centers for clusters:
 
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{./figs/dpc_3.jpg}
	\caption{Inside decision graph: $\rho = \rho_{0}$ and $\delta = \delta_{0}$ }: Points with a high local density (located on the right side of the x-axis). Points that are far from any other point with a higher density (located at the top of the y-axis). Source \cite{DPC:2014}.
	\label{fig:mst}
	\end{figure}

\subsection{Previous machine learning applications in galaxy clustering}

This section briefly reviews several Machine Learning (ML) applications in cosmology, with particular emphasis on clustering techniques. But first we will introduce some historical research in globally galaxy custering.

In 18th centuty Charles Messier and William Herschel noted a concentration of nebulae (we know today that are large galaxies) in Virgo and Coma constellations \cite{ostriker:2014}.

In the 1920s Edwin Hubble proved that spiral and elliptical nebulae were extragalactic systems (galaxies) far outside the Milky Way \cite{ostriker:2014}.

In 1937 Friz Zwicky published the article \textit{On the Masses of Nebulae and of Clusters of Nebulae}. This was a study about the velocity dispersion of galaxies in the Coma cluster. In this work he showed that the galaxies were moving too fast to be held together by the visible matter, leading to the first evidence and postulation of dark matter to explain the cluster's stability \cite{zwicky:1937}.

The first systematic, statistically complete catalog compiled by George Abell in 1958 became the foundation for modern cluster studies, allowing for a rigorous, statistical analysis of galaxy clustering across large volumes.

Several works in the literature use supervised methods. For example, Thomas et al. \cite{Thomas:2025} generate predictive regression models based on the MACSIS simulation to predict cluster features from specific observables. On the other hand, Sadikov et al. \cite{Sadikov:2025} present an analysis of the X-ray properties of the galaxy cluster population in the $z=0$ snapshot of the IllustrisTNG simulations, utilizing machine learning to perform clustering and regression tasks.

In contrast, other studies applying Machine Learning (ML) to the galactic Universe directly address the intrinsic properties of galaxies rather than focus on the clustering problem. For example, Dvorkin et al. \cite{Dvorkin:2022} note that "it has been shown that unknown relations between galaxy properties and parameters describing the composition of the Universe can be easily identified by employing machine learning techniques on top of state-of-the-art hydrodynamic simulations" \cite{Villaescusa-Navarro:2022}.

The most significant application of density-based algorithms to galaxy distribution is a recent article (dated 2025) by Hai-Xia-Ma et al.\cite{Hai-Xia-Ma:2025}. The authors successfully applied density-based algorithms, including a modified version called sOPTICS, to several galaxy catalogs, achieving a notable success in cluster detection. They created the modified version of OPTICS called sOPTICS and used it to mitigate the redshift space distortion along line-of-sight caused by galaxies' peculiar velocities.


As the reader can observe, a gap currently persists in the astronomical literature regarding the widespread application of unsupervised density-based algorithms for the systematic detection of galaxy groups and clusters. This limited exploration of density-based techniques, particularly in validating existing catalogs, underscores the novelty of this work. Furthermore, the large-scale distribution of matter across the Universe presents several fundamental problems that lie at the frontier of modern physics, such as understanding the nature of dark matter and dark energy. By providing robust, objective characterizations of cosmic structures across all scales, this study contributes essential input for constraining cosmological models and addressing these profound mysteries.
